O MapReduce é um modelo de programação que permite o processamento de dados massivos em um algoritmo paralelo e distribuído, geralmente em um cluster de computadores. Hoje, o Hadoop é utilizado em larga escala por grandes corporações, como Facebook e Twitter, em aplicações Big Data. Este tema será útil para aplicações que envolvam dados massivos para processamento paralelo (embora seja interessante para processamento de quaisquer dados), geralmente utilizando um cluster de computadores.

A capacidade dos discos rígidos e outros elementos de armazenamento aumentaram bastante nos últimos anos, mas a velocidade de leitura e escrita dos mesmos não acompanhou o mesmo ritmo. Como um exemplo, a leitura de todo um disco rígido 20 anos atrás levava cerca de cinco minutos. Atualmente, leva mais de duas horas e meia. Trata-se de um longo período para ler todos os dados, e escrever é ainda mais lento. A solução mais óbvia para resolver esse problema é ler/escrever os dados em paralelo, utilizando vários discos. Deste modo, se existem 100 HDs, cada um com 1% do total dos dados, por exemplo, a leitura pode ser realizada 100 vezes mais rapidamente, em teoria.

No caso dessa leitura/escrita paralela dos dados, dois problemas são bastante comuns. O primeiro é bastante óbvio: se há 100 vezes mais discos rígidos, a chance de existir falha em um deles é 100 vezes maior, o que pode ocasionar perda de dados. Para evitar esse problema, geralmente utiliza-se a replicação, onde cópias de segurança dos dados são mantidas em diferentes discos. Outro problema é que muitas tarefas de análise de dados necessitam combinar dados “espalhados” em discos diferentes. Entretanto, esses problemas não geram dores de cabeça aos programadores, pois o MapReduce oferece um modelo de programação que os abstrai, uma vez que o processamento é realizado através de uma combinação entre chaves e valores (keys e values) que podem estar em diferentes discos.

No meio da análise de dados, um conceito que ganha força, e no qual grande parte do MapReduce está baseado, é o Big Data. Trata-se de um termo empregado para descrever o crescimento, o uso e a disponibilidade das informações, sejam elas estruturadas ou não. Para o Big Data, o importante não é a coleta de grandes quantidades de dados, mas sim como eles são processados. O potencial que ele traz para as empresas é imenso e para utilizá-lo, elas precisam ser capazes de aproveitar as informações contidas em suas gigantescas bases de dados para tomar as melhores decisões. Em outras palavras, Big Data não se refere apenas aos dados, mas também às soluções tecnológicas criadas para lidar com esses dados em quantidade, variedade e velocidade bastante significativos.

Nesse cenário, é praticamente impossível falar de MapReduce e esquecer do Hadoop. Basicamente, foi ele quem trouxe o MapReduce como solução para o processamento paralelo de dados e deu a ele o status que tem hoje. O Hadoop é um projeto que oferece uma solução para problemas relacionados à Big Data, tendo em seu núcleo duas partes essenciais: o Hadoop Distributed Filesystem (HDFS), que é um sistema de arquivos distribuído e confiável, responsável pelo armazenamento dos dados, e o próprio Hadoop MapReduce, responsável pela análise e processamento dos dados. Ambos possuem a confiabilidade como uma marca, o que torna o sistema muito robusto para aplicações que envolvem dados massivos e importantes para as organizações que o utilizam.

Fonte: https://www.devmedia.com.br/hadoop-mapreduce-introducao-a-big-data/30034